{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ebd6ae81-2410-4bfa-91f2-c0c2f3169142",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:41: SyntaxWarning: invalid escape sequence '\\$'\n",
      "<>:42: SyntaxWarning: invalid escape sequence '\\$'\n",
      "<>:41: SyntaxWarning: invalid escape sequence '\\$'\n",
      "<>:42: SyntaxWarning: invalid escape sequence '\\$'\n",
      "C:\\Users\\shiva\\AppData\\Local\\Temp\\ipykernel_11016\\1418124155.py:41: SyntaxWarning: invalid escape sequence '\\$'\n",
      "  df_type_clean['price'] = df_type_clean['price'].replace('[\\$,]', '', regex=True).astype(float)\n",
      "C:\\Users\\shiva\\AppData\\Local\\Temp\\ipykernel_11016\\1418124155.py:42: SyntaxWarning: invalid escape sequence '\\$'\n",
      "  df_type_clean['service fee'] = df_type_clean['service fee'].replace('[\\$,]', '', regex=True).astype(float)\n",
      "C:\\Users\\shiva\\AppData\\Local\\Temp\\ipykernel_11016\\1418124155.py:22: DtypeWarning: Columns (25) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  airbnb = pd.read_csv(\"Airbnb_Open_Data.csv\", header=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "host_identity_verified     object\n",
      "neighbourhood group        object\n",
      "instant_bookable           object\n",
      "cancellation_policy        object\n",
      "room type                  object\n",
      "Construction year         float64\n",
      "price                      object\n",
      "service fee                object\n",
      "minimum nights            float64\n",
      "number of reviews         float64\n",
      "last review                object\n",
      "reviews per month         float64\n",
      "review rate number        float64\n",
      "availability 365          float64\n",
      "dtype: object\n",
      "host_identity_verified     object\n",
      "neighbourhood group        object\n",
      "instant_bookable           object\n",
      "cancellation_policy        object\n",
      "room type                  object\n",
      "Construction year           int32\n",
      "price                      object\n",
      "service fee                object\n",
      "minimum nights              int32\n",
      "number of reviews           int32\n",
      "last review                object\n",
      "reviews per month         float64\n",
      "review rate number          int32\n",
      "availability 365            int32\n",
      "dtype: object\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 85507 entries, 0 to 102597\n",
      "Data columns (total 14 columns):\n",
      " #   Column                  Non-Null Count  Dtype         \n",
      "---  ------                  --------------  -----         \n",
      " 0   host_identity_verified  85507 non-null  category      \n",
      " 1   neighbourhood group     85498 non-null  category      \n",
      " 2   instant_bookable        85445 non-null  category      \n",
      " 3   cancellation_policy     85453 non-null  category      \n",
      " 4   room type               85507 non-null  category      \n",
      " 5   Construction year       85507 non-null  int32         \n",
      " 6   price                   85294 non-null  float64       \n",
      " 7   service fee             85271 non-null  float64       \n",
      " 8   minimum nights          85507 non-null  int32         \n",
      " 9   number of reviews       85507 non-null  int32         \n",
      " 10  last review             85507 non-null  datetime64[ns]\n",
      " 11  reviews per month       85507 non-null  float64       \n",
      " 12  review rate number      85507 non-null  int32         \n",
      " 13  availability 365        85507 non-null  int32         \n",
      "dtypes: category(5), datetime64[ns](1), float64(3), int32(5)\n",
      "memory usage: 5.3 MB\n",
      "None\n",
      "       host_identity_verified neighbourhood group instant_bookable  \\\n",
      "count                   85507               85498            85445   \n",
      "unique                      2                   7                2   \n",
      "top               unconfirmed            Brooklyn            False   \n",
      "freq                    42773               35493            42931   \n",
      "mean                      NaN                 NaN              NaN   \n",
      "min                       NaN                 NaN              NaN   \n",
      "25%                       NaN                 NaN              NaN   \n",
      "50%                       NaN                 NaN              NaN   \n",
      "75%                       NaN                 NaN              NaN   \n",
      "max                       NaN                 NaN              NaN   \n",
      "std                       NaN                 NaN              NaN   \n",
      "\n",
      "       cancellation_policy        room type  Construction year         price  \\\n",
      "count                85453            85507       85507.000000  85294.000000   \n",
      "unique                   3                4                NaN           NaN   \n",
      "top               moderate  Entire home/apt                NaN           NaN   \n",
      "freq                 28627            45228                NaN           NaN   \n",
      "mean                   NaN              NaN        2012.488077    626.168417   \n",
      "min                    NaN              NaN        2003.000000     50.000000   \n",
      "25%                    NaN              NaN        2007.000000    340.000000   \n",
      "50%                    NaN              NaN        2012.000000    625.000000   \n",
      "75%                    NaN              NaN        2017.000000    914.000000   \n",
      "max                    NaN              NaN        2022.000000   1200.000000   \n",
      "std                    NaN              NaN           5.762688    331.680496   \n",
      "\n",
      "         service fee  minimum nights  number of reviews  \\\n",
      "count   85271.000000    85507.000000       85507.000000   \n",
      "unique           NaN             NaN                NaN   \n",
      "top              NaN             NaN                NaN   \n",
      "freq             NaN             NaN                NaN   \n",
      "mean      125.196210        7.458711          32.191879   \n",
      "min        10.000000     -365.000000           1.000000   \n",
      "25%        68.000000        2.000000           3.000000   \n",
      "50%       125.000000        3.000000          11.000000   \n",
      "75%       183.000000        5.000000          38.000000   \n",
      "max       240.000000     5645.000000        1024.000000   \n",
      "std        66.332006       27.933355          51.742288   \n",
      "\n",
      "                          last review  reviews per month  review rate number  \\\n",
      "count                           85507       85507.000000        85507.000000   \n",
      "unique                            NaN                NaN                 NaN   \n",
      "top                               NaN                NaN                 NaN   \n",
      "freq                              NaN                NaN                 NaN   \n",
      "mean    2019-06-11 23:36:11.232764672           1.371968            3.280012   \n",
      "min               2012-07-11 00:00:00           0.000000            1.000000   \n",
      "25%               2018-10-28 00:00:00           0.220000            2.000000   \n",
      "50%               2019-06-13 00:00:00           0.740000            3.000000   \n",
      "75%               2019-07-05 00:00:00           2.000000            4.000000   \n",
      "max               2026-03-28 00:00:00          90.000000            5.000000   \n",
      "std                               NaN           1.743212            1.282919   \n",
      "\n",
      "        availability 365  \n",
      "count       85507.000000  \n",
      "unique               NaN  \n",
      "top                  NaN  \n",
      "freq                 NaN  \n",
      "mean          141.825102  \n",
      "min           -10.000000  \n",
      "25%             6.000000  \n",
      "50%           101.000000  \n",
      "75%           266.000000  \n",
      "max          3677.000000  \n",
      "std           133.974763  \n",
      "50.0\n",
      "host_identity_verified                        int64\n",
      "cancellation_policy                           int64\n",
      "Construction year                             int32\n",
      "price                                       float64\n",
      "service fee                                 float64\n",
      "minimum nights                                int32\n",
      "number of reviews                             int32\n",
      "reviews per month                           float64\n",
      "review rate number                            int32\n",
      "availability 365                              int32\n",
      "neighbourhood group_Brooklyn                   bool\n",
      "neighbourhood group_Manhattan                  bool\n",
      "neighbourhood group_Queens                     bool\n",
      "neighbourhood group_Staten Island              bool\n",
      "neighbourhood group_brookln                    bool\n",
      "neighbourhood group_manhatan                   bool\n",
      "room type_Hotel room                           bool\n",
      "room type_Private room                         bool\n",
      "room type_Shared room                          bool\n",
      "instant_bookable_True                          bool\n",
      "last_review                          datetime64[ns]\n",
      "review_year                                   int32\n",
      "review_month                                  int32\n",
      "dtype: object\n",
      "Non-numeric columns: []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shiva\\AppData\\Local\\Temp\\ipykernel_11016\\1418124155.py:69: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['host_identity_verified'] = data['host_identity_verified'].astype(str)\n",
      "C:\\Users\\shiva\\AppData\\Local\\Temp\\ipykernel_11016\\1418124155.py:70: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['host_identity_verified'] = data['host_identity_verified'].map(\n",
      "C:\\Users\\shiva\\AppData\\Local\\Temp\\ipykernel_11016\\1418124155.py:76: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['cancellation_policy'] = data['cancellation_policy'].astype(str)\n",
      "C:\\Users\\shiva\\AppData\\Local\\Temp\\ipykernel_11016\\1418124155.py:78: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['cancellation_policy'] = data['cancellation_policy'].map(cancellation_policy_mapping)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.inspection import permutation_importance\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, classification_report\n",
    "from scipy.stats import pointbiserialr\n",
    "\n",
    "# airbnb = pd.read_csv(\"/content/Airbnb_Open_Data.csv\", header=0)\n",
    "airbnb = pd.read_csv(\"Airbnb_Open_Data.csv\", header=0)\n",
    "airbnb.head()\n",
    "df_subset = airbnb.drop(['id', 'NAME', 'neighbourhood', 'house_rules', 'host name', 'host id', 'lat', 'long', 'country',\n",
    "                         'calculated host listings count', 'country code', 'license'], axis=1)\n",
    "df_subset.head()\n",
    "\n",
    "\n",
    "print(df_subset.dtypes) # checked to see types --> a few that should be int are float\n",
    "\n",
    "df_subset = df_subset.dropna(subset=['Construction year', 'minimum nights', 'number of reviews', 'review rate number', 'availability 365'])\n",
    "\n",
    "df_type_clean = df_subset.astype({'Construction year': 'int32',\n",
    "                                  'minimum nights': 'int32',\n",
    "                                  'number of reviews': 'int32',\n",
    "                                  'review rate number': 'int32',\n",
    "                                  'availability 365': 'int32'})\n",
    "\n",
    "print(df_type_clean.dtypes)\n",
    "# Remove any non-numeric characters (e.g., $) and convert to float\n",
    "df_type_clean['price'] = df_type_clean['price'].replace('[\\$,]', '', regex=True).astype(float)\n",
    "df_type_clean['service fee'] = df_type_clean['service fee'].replace('[\\$,]', '', regex=True).astype(float)\n",
    "\n",
    "df_type_clean['reviews per month'] = df_type_clean['reviews per month'].fillna(0) # Fill with appropriate defaults\n",
    "df_type_clean = df_type_clean.dropna(subset=['last review', 'host_identity_verified']) # Drop rows with critical missing data\n",
    "\n",
    "df_type_clean['last review'] = pd.to_datetime(df_type_clean['last review'], errors='coerce') #Convert last review to a datetime object\n",
    "\n",
    "#Convert categorical columns (host_identity_verified, neighbourhood group, room type, etc.) to category\n",
    "categorical_columns = ['host_identity_verified', 'neighbourhood group', 'instant_bookable', 'cancellation_policy', 'room type']\n",
    "for col in categorical_columns:\n",
    "    df_type_clean[col] = df_type_clean[col].astype('category')\n",
    "\n",
    "#df_type_clean['house_rules'] = df_type_clean['house_rules'].str.strip() #Clean up text columns like house_rules to remove extra spaces or inconsistent casing\n",
    "#df_type_clean['house_rules'] = df_type_clean['house_rules'].replace('#NAME?', np.nan) #this was an excel error I believe --> wanted to get rid of it\n",
    "\n",
    "print(df_type_clean.info())\n",
    "print(df_type_clean.describe(include='all'))\n",
    "# wanted to ensure that all of the prices had to be greater than 0\n",
    "filter(df_type_clean, df_type_clean['price'] > 0)\n",
    "print(df_type_clean['price'].min())\n",
    "# for a few of the built-in functions, we can't have any NaN values\n",
    "# so, we will create this new Pandas DataFrame, but keep our old one for other analyis\n",
    "df_cleaned = df_type_clean.dropna()\n",
    "\n",
    "\n",
    "# Load data\n",
    "data = df_cleaned\n",
    "data['host_identity_verified'] = data['host_identity_verified'].astype(str)\n",
    "data['host_identity_verified'] = data['host_identity_verified'].map(\n",
    "    {'unconfirmed': 0, 'verified': 1, '0': 0, '1': 1})\n",
    "\n",
    "\n",
    "# One-hot encode 'neighbourhood group' and 'room type'\n",
    "#data = pd.get_dummies(data, columns=['neighbourhood group', 'room type', 'cancellation_policy'], drop_first=True)\n",
    "data['cancellation_policy'] = data['cancellation_policy'].astype(str)\n",
    "cancellation_policy_mapping = {'strict': 0, 'moderate': 1, 'flexible': 2}\n",
    "data['cancellation_policy'] = data['cancellation_policy'].map(cancellation_policy_mapping)\n",
    "\n",
    "data = pd.get_dummies(data, columns=['neighbourhood group', 'room type', 'instant_bookable'], drop_first=True)\n",
    "\n",
    "# 4. Handle the 'last review' column (extract year and month)\n",
    "data['last_review'] = pd.to_datetime(data['last review'], errors='coerce')\n",
    "data['review_year'] = data['last_review'].dt.year\n",
    "data['review_month'] = data['last_review'].dt.month\n",
    "data.drop(['last review'], axis=1, inplace=True)\n",
    "print(data.dtypes)\n",
    "\n",
    "# Preprocessing: Transform the dataset\n",
    "# Ensure datetime is converted to numeric and drop non-numeric columns\n",
    "data['days_since_last_review'] = (pd.to_datetime('today') - data['last_review']).dt.days\n",
    "data.drop(columns=[ 'last_review','predicted_price', 'price_category'], inplace=True, errors='ignore')  # Drop non-numeric columns\n",
    "bool_columns = data.select_dtypes(include='bool').columns\n",
    "data[bool_columns] = data[bool_columns].astype(int)\n",
    "\n",
    "# Check for non-numeric columns\n",
    "non_numeric_cols = data.select_dtypes(include=['object']).columns\n",
    "print(f\"Non-numeric columns: {list(non_numeric_cols)}\")  # Ensure no strings remain\n",
    "assert len(non_numeric_cols) == 0, \"Dataset contains non-numeric columns!\"\n",
    "\n",
    "# Separate features and target variable\n",
    "X = data.drop(columns=['price','service fee'])\n",
    "y = data['price']\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b6ffde09-1d07-4fde-a103-35ef0518aebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1913/1913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - loss: 163124.2344 - mse: 163124.2344 - val_loss: 112315.3594 - val_mse: 112315.3594\n",
      "Epoch 2/100\n",
      "\u001b[1m1913/1913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 111247.3750 - mse: 111247.3750 - val_loss: 112963.7266 - val_mse: 112963.7266\n",
      "Epoch 3/100\n",
      "\u001b[1m1913/1913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 111163.8438 - mse: 111163.8438 - val_loss: 111798.4844 - val_mse: 111798.4844\n",
      "Epoch 4/100\n",
      "\u001b[1m1913/1913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 110287.1094 - mse: 110287.1094 - val_loss: 113298.1094 - val_mse: 113298.1094\n",
      "Epoch 5/100\n",
      "\u001b[1m1913/1913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 110591.6172 - mse: 110591.6172 - val_loss: 112084.8516 - val_mse: 112084.8516\n",
      "Epoch 6/100\n",
      "\u001b[1m1913/1913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 110611.0469 - mse: 110611.0469 - val_loss: 111706.0938 - val_mse: 111706.0938\n",
      "Epoch 7/100\n",
      "\u001b[1m1913/1913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 110590.6953 - mse: 110590.6953 - val_loss: 112375.1562 - val_mse: 112375.1562\n",
      "Epoch 8/100\n",
      "\u001b[1m1913/1913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - loss: 109798.7891 - mse: 109798.7891 - val_loss: 112186.6172 - val_mse: 112186.6172\n",
      "Epoch 9/100\n",
      "\u001b[1m1913/1913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 109700.7188 - mse: 109700.7188 - val_loss: 112604.8516 - val_mse: 112604.8516\n",
      "Epoch 10/100\n",
      "\u001b[1m1913/1913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 110392.2969 - mse: 110392.2969 - val_loss: 111754.4688 - val_mse: 111754.4688\n",
      "Epoch 11/100\n",
      "\u001b[1m1913/1913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 110604.5234 - mse: 110604.5234 - val_loss: 112037.2891 - val_mse: 112037.2891\n",
      "Epoch 12/100\n",
      "\u001b[1m1913/1913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 110485.8594 - mse: 110485.8594 - val_loss: 111523.3047 - val_mse: 111523.3047\n",
      "Epoch 13/100\n",
      "\u001b[1m1913/1913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 109646.5859 - mse: 109646.5859 - val_loss: 111451.8125 - val_mse: 111451.8125\n",
      "Epoch 14/100\n",
      "\u001b[1m1913/1913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 108199.6953 - mse: 108199.6953 - val_loss: 112034.8359 - val_mse: 112034.8359\n",
      "Epoch 15/100\n",
      "\u001b[1m1913/1913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 109928.2266 - mse: 109928.2266 - val_loss: 111586.1328 - val_mse: 111586.1328\n",
      "Epoch 16/100\n",
      "\u001b[1m1913/1913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 109548.9922 - mse: 109548.9922 - val_loss: 111583.4453 - val_mse: 111583.4453\n",
      "Epoch 17/100\n",
      "\u001b[1m1913/1913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 109405.8672 - mse: 109405.8672 - val_loss: 112972.8516 - val_mse: 112972.8516\n",
      "Epoch 18/100\n",
      "\u001b[1m1913/1913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 110068.5391 - mse: 110068.5391 - val_loss: 112268.9688 - val_mse: 112268.9688\n",
      "Epoch 19/100\n",
      "\u001b[1m1913/1913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 109301.8359 - mse: 109301.8359 - val_loss: 112582.1250 - val_mse: 112582.1250\n",
      "Epoch 20/100\n",
      "\u001b[1m1913/1913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 109211.7891 - mse: 109211.7891 - val_loss: 113630.1641 - val_mse: 113630.1641\n",
      "Epoch 21/100\n",
      "\u001b[1m1913/1913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 109186.8594 - mse: 109186.8594 - val_loss: 112058.6953 - val_mse: 112058.6953\n",
      "Epoch 22/100\n",
      "\u001b[1m1913/1913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 109766.9766 - mse: 109766.9766 - val_loss: 111625.9766 - val_mse: 111625.9766\n",
      "Epoch 23/100\n",
      "\u001b[1m1913/1913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 108872.7266 - mse: 108872.7266 - val_loss: 111908.3438 - val_mse: 111908.3438\n",
      "Epoch 24/100\n",
      "\u001b[1m1913/1913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 109122.2031 - mse: 109122.2031 - val_loss: 111881.0781 - val_mse: 111881.0781\n",
      "Epoch 25/100\n",
      "\u001b[1m1913/1913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 109189.7031 - mse: 109189.7031 - val_loss: 111571.9219 - val_mse: 111571.9219\n",
      "Epoch 26/100\n",
      "\u001b[1m1913/1913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 108804.7188 - mse: 108804.7188 - val_loss: 111404.5625 - val_mse: 111404.5625\n",
      "Epoch 27/100\n",
      "\u001b[1m1913/1913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 108433.6172 - mse: 108433.6172 - val_loss: 112124.8359 - val_mse: 112124.8359\n",
      "Epoch 28/100\n",
      "\u001b[1m1913/1913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 109115.9297 - mse: 109115.9297 - val_loss: 113389.0469 - val_mse: 113389.0469\n",
      "Epoch 29/100\n",
      "\u001b[1m1913/1913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 108841.0000 - mse: 108841.0000 - val_loss: 111818.3906 - val_mse: 111818.3906\n",
      "Epoch 30/100\n",
      "\u001b[1m1913/1913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 108290.3438 - mse: 108290.3438 - val_loss: 112018.8594 - val_mse: 112018.8594\n",
      "Epoch 31/100\n",
      "\u001b[1m1913/1913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 108976.3828 - mse: 108976.3828 - val_loss: 112907.9375 - val_mse: 112907.9375\n",
      "Epoch 32/100\n",
      "\u001b[1m1913/1913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 109386.4062 - mse: 109386.4062 - val_loss: 111657.0078 - val_mse: 111657.0078\n",
      "Epoch 33/100\n",
      "\u001b[1m1913/1913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 109257.0781 - mse: 109257.0781 - val_loss: 112896.9688 - val_mse: 112896.9688\n",
      "Epoch 34/100\n",
      "\u001b[1m1913/1913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 109196.6797 - mse: 109196.6797 - val_loss: 111815.5156 - val_mse: 111815.5156\n",
      "Epoch 35/100\n",
      "\u001b[1m1913/1913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - loss: 109032.3828 - mse: 109032.3828 - val_loss: 112410.3438 - val_mse: 112410.3438\n",
      "Epoch 36/100\n",
      "\u001b[1m1913/1913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 107963.8203 - mse: 107963.8203 - val_loss: 111772.3203 - val_mse: 111772.3203\n",
      "Epoch 37/100\n",
      "\u001b[1m1913/1913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 108510.6953 - mse: 108510.6953 - val_loss: 112124.0781 - val_mse: 112124.0781\n",
      "Epoch 38/100\n",
      "\u001b[1m1913/1913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 108281.1328 - mse: 108281.1328 - val_loss: 111815.4766 - val_mse: 111815.4766\n",
      "Epoch 39/100\n",
      "\u001b[1m1913/1913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 108359.7109 - mse: 108359.7109 - val_loss: 112305.9922 - val_mse: 112305.9922\n",
      "Epoch 40/100\n",
      "\u001b[1m1913/1913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 107992.5625 - mse: 107992.5625 - val_loss: 112855.7734 - val_mse: 112855.7734\n",
      "Epoch 41/100\n",
      "\u001b[1m1913/1913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 107624.5391 - mse: 107624.5391 - val_loss: 112637.3828 - val_mse: 112637.3828\n",
      "Epoch 42/100\n",
      "\u001b[1m1913/1913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 108908.1719 - mse: 108908.1719 - val_loss: 112629.4609 - val_mse: 112629.4609\n",
      "Epoch 43/100\n",
      "\u001b[1m1913/1913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 107516.5078 - mse: 107516.5078 - val_loss: 112363.9609 - val_mse: 112363.9609\n",
      "Epoch 44/100\n",
      "\u001b[1m1913/1913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 108134.6172 - mse: 108134.6172 - val_loss: 112739.0547 - val_mse: 112739.0547\n",
      "Epoch 45/100\n",
      "\u001b[1m1913/1913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 107811.3516 - mse: 107811.3516 - val_loss: 112590.3047 - val_mse: 112590.3047\n",
      "Epoch 46/100\n",
      "\u001b[1m1913/1913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 107993.5156 - mse: 107993.5156 - val_loss: 112385.7422 - val_mse: 112385.7422\n",
      "Epoch 47/100\n",
      "\u001b[1m1913/1913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 106768.0312 - mse: 106768.0312 - val_loss: 112322.3828 - val_mse: 112322.3828\n",
      "Epoch 48/100\n",
      "\u001b[1m1913/1913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 107397.0156 - mse: 107397.0156 - val_loss: 112620.3906 - val_mse: 112620.3906\n",
      "Epoch 49/100\n",
      "\u001b[1m1913/1913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 108137.6406 - mse: 108137.6406 - val_loss: 113166.0547 - val_mse: 113166.0547\n",
      "Epoch 50/100\n",
      "\u001b[1m1913/1913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 107180.3047 - mse: 107180.3047 - val_loss: 113384.5938 - val_mse: 113384.5938\n",
      "Epoch 51/100\n",
      "\u001b[1m1913/1913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 107349.1172 - mse: 107349.1172 - val_loss: 113563.3672 - val_mse: 113563.3672\n",
      "Epoch 52/100\n",
      "\u001b[1m1913/1913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 107840.6406 - mse: 107840.6406 - val_loss: 112958.0938 - val_mse: 112958.0938\n",
      "Epoch 53/100\n",
      "\u001b[1m1913/1913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 106887.4453 - mse: 106887.4453 - val_loss: 114392.9922 - val_mse: 114392.9922\n",
      "Epoch 54/100\n",
      "\u001b[1m1913/1913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 106951.3750 - mse: 106951.3750 - val_loss: 113396.2578 - val_mse: 113396.2578\n",
      "Epoch 55/100\n",
      "\u001b[1m1913/1913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 107092.1953 - mse: 107092.1953 - val_loss: 114098.1797 - val_mse: 114098.1797\n",
      "Epoch 56/100\n",
      "\u001b[1m1913/1913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 107424.6406 - mse: 107424.6406 - val_loss: 113583.5078 - val_mse: 113583.5078\n",
      "Epoch 57/100\n",
      "\u001b[1m1913/1913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 106860.1016 - mse: 106860.1016 - val_loss: 113607.7891 - val_mse: 113607.7891\n",
      "Epoch 58/100\n",
      "\u001b[1m1913/1913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 106127.1797 - mse: 106127.1797 - val_loss: 113728.5078 - val_mse: 113728.5078\n",
      "Epoch 59/100\n",
      "\u001b[1m1913/1913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 106384.2500 - mse: 106384.2500 - val_loss: 113894.4375 - val_mse: 113894.4375\n",
      "Epoch 60/100\n",
      "\u001b[1m1913/1913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - loss: 105741.1484 - mse: 105741.1484 - val_loss: 113983.5078 - val_mse: 113983.5078\n",
      "Epoch 61/100\n",
      "\u001b[1m1913/1913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 105810.7422 - mse: 105810.7422 - val_loss: 114826.6641 - val_mse: 114826.6641\n",
      "Epoch 62/100\n",
      "\u001b[1m1913/1913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 106354.4219 - mse: 106354.4219 - val_loss: 115351.8828 - val_mse: 115351.8828\n",
      "Epoch 63/100\n",
      "\u001b[1m1913/1913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 106516.3906 - mse: 106516.3906 - val_loss: 114315.8438 - val_mse: 114315.8438\n",
      "Epoch 64/100\n",
      "\u001b[1m1913/1913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 106080.2812 - mse: 106080.2812 - val_loss: 114228.2891 - val_mse: 114228.2891\n",
      "Epoch 65/100\n",
      "\u001b[1m1913/1913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 105434.1094 - mse: 105434.1094 - val_loss: 114707.7344 - val_mse: 114707.7344\n",
      "Epoch 66/100\n",
      "\u001b[1m1913/1913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 106194.7188 - mse: 106194.7188 - val_loss: 115033.5859 - val_mse: 115033.5859\n",
      "Epoch 67/100\n",
      "\u001b[1m1913/1913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 104860.9219 - mse: 104860.9219 - val_loss: 114347.5703 - val_mse: 114347.5703\n",
      "Epoch 68/100\n",
      "\u001b[1m1913/1913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 104477.3750 - mse: 104477.3750 - val_loss: 115945.4219 - val_mse: 115945.4219\n",
      "Epoch 69/100\n",
      "\u001b[1m1913/1913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 105123.0000 - mse: 105123.0000 - val_loss: 114989.9453 - val_mse: 114989.9453\n",
      "Epoch 70/100\n",
      "\u001b[1m1913/1913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 105212.3125 - mse: 105212.3125 - val_loss: 115532.1875 - val_mse: 115532.1875\n",
      "Epoch 71/100\n",
      "\u001b[1m1913/1913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 104806.6641 - mse: 104806.6641 - val_loss: 115344.4766 - val_mse: 115344.4766\n",
      "Epoch 72/100\n",
      "\u001b[1m1913/1913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 104407.8750 - mse: 104407.8750 - val_loss: 115182.2578 - val_mse: 115182.2578\n",
      "Epoch 73/100\n",
      "\u001b[1m1913/1913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 104454.8359 - mse: 104454.8359 - val_loss: 115696.6719 - val_mse: 115696.6719\n",
      "Epoch 74/100\n",
      "\u001b[1m1913/1913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 104124.2188 - mse: 104124.2188 - val_loss: 115730.6875 - val_mse: 115730.6875\n",
      "Epoch 75/100\n",
      "\u001b[1m1913/1913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 104288.7188 - mse: 104288.7188 - val_loss: 115409.5781 - val_mse: 115409.5781\n",
      "Epoch 76/100\n",
      "\u001b[1m1913/1913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 103920.0625 - mse: 103920.0625 - val_loss: 115760.8906 - val_mse: 115760.8906\n",
      "Epoch 77/100\n",
      "\u001b[1m1913/1913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 103660.2500 - mse: 103660.2500 - val_loss: 116356.1016 - val_mse: 116356.1016\n",
      "Epoch 78/100\n",
      "\u001b[1m1913/1913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 102937.1250 - mse: 102937.1250 - val_loss: 115754.4375 - val_mse: 115754.4375\n",
      "Epoch 79/100\n",
      "\u001b[1m1913/1913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 103795.3984 - mse: 103795.3984 - val_loss: 116296.1875 - val_mse: 116296.1875\n",
      "Epoch 80/100\n",
      "\u001b[1m1913/1913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 103385.8828 - mse: 103385.8828 - val_loss: 116294.9062 - val_mse: 116294.9062\n",
      "Epoch 81/100\n",
      "\u001b[1m1913/1913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 103148.9531 - mse: 103148.9531 - val_loss: 116588.4453 - val_mse: 116588.4453\n",
      "Epoch 82/100\n",
      "\u001b[1m1913/1913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - loss: 103079.3750 - mse: 103079.3750 - val_loss: 117327.4531 - val_mse: 117327.4531\n",
      "Epoch 83/100\n",
      "\u001b[1m1913/1913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 102175.6797 - mse: 102175.6797 - val_loss: 118352.8672 - val_mse: 118352.8672\n",
      "Epoch 84/100\n",
      "\u001b[1m1913/1913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 102347.2344 - mse: 102347.2344 - val_loss: 117227.1953 - val_mse: 117227.1953\n",
      "Epoch 85/100\n",
      "\u001b[1m1913/1913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 102360.6953 - mse: 102360.6953 - val_loss: 117639.7031 - val_mse: 117639.7031\n",
      "Epoch 86/100\n",
      "\u001b[1m1913/1913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 102495.5078 - mse: 102495.5078 - val_loss: 117536.2422 - val_mse: 117536.2422\n",
      "Epoch 87/100\n",
      "\u001b[1m1913/1913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 101679.2891 - mse: 101679.2891 - val_loss: 117642.5469 - val_mse: 117642.5469\n",
      "Epoch 88/100\n",
      "\u001b[1m1913/1913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 101224.5312 - mse: 101224.5312 - val_loss: 117655.7422 - val_mse: 117655.7422\n",
      "Epoch 89/100\n",
      "\u001b[1m1913/1913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 101873.4297 - mse: 101873.4297 - val_loss: 118080.2734 - val_mse: 118080.2734\n",
      "Epoch 90/100\n",
      "\u001b[1m1913/1913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 101362.2969 - mse: 101362.2969 - val_loss: 118087.9219 - val_mse: 118087.9219\n",
      "Epoch 91/100\n",
      "\u001b[1m1913/1913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 101257.1953 - mse: 101257.1953 - val_loss: 119483.0312 - val_mse: 119483.0312\n",
      "Epoch 92/100\n",
      "\u001b[1m1913/1913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 100630.3594 - mse: 100630.3594 - val_loss: 119276.3438 - val_mse: 119276.3438\n",
      "Epoch 93/100\n",
      "\u001b[1m1913/1913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 100008.3047 - mse: 100008.3047 - val_loss: 119244.0312 - val_mse: 119244.0312\n",
      "Epoch 94/100\n",
      "\u001b[1m1913/1913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 100331.6250 - mse: 100331.6250 - val_loss: 118438.2734 - val_mse: 118438.2734\n",
      "Epoch 95/100\n",
      "\u001b[1m1913/1913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 100010.4688 - mse: 100010.4688 - val_loss: 118605.7812 - val_mse: 118605.7812\n",
      "Epoch 96/100\n",
      "\u001b[1m1913/1913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 99913.0234 - mse: 99913.0234 - val_loss: 118776.1406 - val_mse: 118776.1406\n",
      "Epoch 97/100\n",
      "\u001b[1m1913/1913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 99999.5000 - mse: 99999.5000 - val_loss: 120063.0000 - val_mse: 120063.0000\n",
      "Epoch 98/100\n",
      "\u001b[1m1913/1913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 99384.3125 - mse: 99384.3125 - val_loss: 121011.0234 - val_mse: 121011.0234\n",
      "Epoch 99/100\n",
      "\u001b[1m1913/1913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 99304.6484 - mse: 99304.6484 - val_loss: 119363.7812 - val_mse: 119363.7812\n",
      "Epoch 100/100\n",
      "\u001b[1m1913/1913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 99163.7969 - mse: 99163.7969 - val_loss: 119315.9297 - val_mse: 119315.9297\n",
      "\u001b[1m532/532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 659us/step\n",
      "Deep Learning Model Evaluation:\n",
      "MSE: 116090.20797856133\n",
      "R²: -0.051537455534660825\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# Build the model\n",
    "model = models.Sequential([\n",
    "    layers.Dense(256, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(1)\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mse'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train_scaled, y_train, validation_split=0.1, epochs=100, batch_size=32, verbose=1)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred_nn = model.predict(X_test_scaled).flatten()\n",
    "mse_nn = mean_squared_error(y_test, y_pred_nn)\n",
    "r2_nn = r2_score(y_test, y_pred_nn)\n",
    "\n",
    "print(f\"Deep Learning Model Evaluation:\\nMSE: {mse_nn}\\nR²: {r2_nn}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0ed11d4f-8f82-4122-9c42-7aaebeed432d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.18.0-cp312-cp312-win_amd64.whl.metadata (3.3 kB)\n",
      "Collecting tensorflow-intel==2.18.0 (from tensorflow)\n",
      "  Downloading tensorflow_intel-2.18.0-cp312-cp312-win_amd64.whl.metadata (4.9 kB)\n",
      "Collecting absl-py>=1.0.0 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Downloading absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting flatbuffers>=24.3.25 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Downloading flatbuffers-24.3.25-py2.py3-none-any.whl.metadata (850 bytes)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Downloading gast-0.6.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting libclang>=13.0.0 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Downloading libclang-18.1.1-py2.py3-none-win_amd64.whl.metadata (5.3 kB)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Downloading opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: packaging in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (24.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (4.25.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (75.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.16.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Downloading termcolor-2.5.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (4.11.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.14.1)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Downloading grpcio-1.68.1-cp312-cp312-win_amd64.whl.metadata (4.0 kB)\n",
      "Collecting tensorboard<2.19,>=2.18 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Downloading tensorboard-2.18.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting keras>=3.5.0 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Downloading keras-3.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.26.4)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (3.11.0)\n",
      "Collecting ml-dtypes<0.5.0,>=0.4.0 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Downloading ml_dtypes-0.4.1-cp312-cp312-win_amd64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.18.0->tensorflow) (0.44.0)\n",
      "Requirement already satisfied: rich in c:\\programdata\\anaconda3\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (13.7.1)\n",
      "Collecting namex (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow)\n",
      "  Downloading namex-0.0.8-py3-none-any.whl.metadata (246 bytes)\n",
      "Collecting optree (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow)\n",
      "  Downloading optree-0.13.1-cp312-cp312-win_amd64.whl.metadata (48 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (2024.8.30)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (3.4.1)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow)\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (3.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (2.1.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (0.1.0)\n",
      "Downloading tensorflow-2.18.0-cp312-cp312-win_amd64.whl (7.5 kB)\n",
      "Downloading tensorflow_intel-2.18.0-cp312-cp312-win_amd64.whl (390.3 MB)\n",
      "   ---------------------------------------- 0.0/390.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/390.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.5/390.3 MB 1.4 MB/s eta 0:04:40\n",
      "   ---------------------------------------- 0.8/390.3 MB 1.5 MB/s eta 0:04:27\n",
      "   ---------------------------------------- 1.0/390.3 MB 1.5 MB/s eta 0:04:23\n",
      "   ---------------------------------------- 1.6/390.3 MB 1.7 MB/s eta 0:03:52\n",
      "   ---------------------------------------- 2.1/390.3 MB 1.8 MB/s eta 0:03:39\n",
      "   ---------------------------------------- 2.6/390.3 MB 1.9 MB/s eta 0:03:26\n",
      "   ---------------------------------------- 3.1/390.3 MB 2.0 MB/s eta 0:03:14\n",
      "   ---------------------------------------- 3.7/390.3 MB 2.1 MB/s eta 0:03:07\n",
      "   ---------------------------------------- 4.5/390.3 MB 2.2 MB/s eta 0:02:54\n",
      "    --------------------------------------- 5.0/390.3 MB 2.3 MB/s eta 0:02:49\n",
      "    --------------------------------------- 5.8/390.3 MB 2.4 MB/s eta 0:02:41\n",
      "    --------------------------------------- 6.6/390.3 MB 2.5 MB/s eta 0:02:34\n",
      "    --------------------------------------- 7.3/390.3 MB 2.6 MB/s eta 0:02:28\n",
      "    --------------------------------------- 8.1/390.3 MB 2.7 MB/s eta 0:02:22\n",
      "    --------------------------------------- 9.2/390.3 MB 2.8 MB/s eta 0:02:17\n",
      "   - -------------------------------------- 10.0/390.3 MB 2.9 MB/s eta 0:02:13\n",
      "   - -------------------------------------- 11.0/390.3 MB 3.0 MB/s eta 0:02:07\n",
      "   - -------------------------------------- 12.1/390.3 MB 3.1 MB/s eta 0:02:02\n",
      "   - -------------------------------------- 13.4/390.3 MB 3.2 MB/s eta 0:01:57\n",
      "   - -------------------------------------- 14.4/390.3 MB 3.3 MB/s eta 0:01:54\n",
      "   - -------------------------------------- 15.5/390.3 MB 3.4 MB/s eta 0:01:51\n",
      "   - -------------------------------------- 16.5/390.3 MB 3.5 MB/s eta 0:01:48\n",
      "   - -------------------------------------- 17.8/390.3 MB 3.6 MB/s eta 0:01:45\n",
      "   - -------------------------------------- 19.1/390.3 MB 3.7 MB/s eta 0:01:42\n",
      "   -- ------------------------------------- 20.4/390.3 MB 3.8 MB/s eta 0:01:38\n",
      "   -- ------------------------------------- 21.8/390.3 MB 3.9 MB/s eta 0:01:36\n",
      "   -- ------------------------------------- 23.1/390.3 MB 4.0 MB/s eta 0:01:33\n",
      "   -- ------------------------------------- 24.6/390.3 MB 4.1 MB/s eta 0:01:30\n",
      "   -- ------------------------------------- 26.0/390.3 MB 4.2 MB/s eta 0:01:28\n",
      "   -- ------------------------------------- 27.5/390.3 MB 4.3 MB/s eta 0:01:26\n",
      "   -- ------------------------------------- 29.1/390.3 MB 4.4 MB/s eta 0:01:23\n",
      "   --- ------------------------------------ 30.9/390.3 MB 4.5 MB/s eta 0:01:21\n",
      "   --- ------------------------------------ 32.8/390.3 MB 4.6 MB/s eta 0:01:18\n",
      "   --- ------------------------------------ 34.1/390.3 MB 4.7 MB/s eta 0:01:17\n",
      "   --- ------------------------------------ 35.9/390.3 MB 4.8 MB/s eta 0:01:15\n",
      "   --- ------------------------------------ 37.5/390.3 MB 4.8 MB/s eta 0:01:13\n",
      "   --- ------------------------------------ 38.8/390.3 MB 4.9 MB/s eta 0:01:12\n",
      "   ---- ----------------------------------- 40.6/390.3 MB 5.0 MB/s eta 0:01:11\n",
      "   ---- ----------------------------------- 42.5/390.3 MB 5.1 MB/s eta 0:01:09\n",
      "   ---- ----------------------------------- 44.6/390.3 MB 5.2 MB/s eta 0:01:07\n",
      "   ---- ----------------------------------- 46.7/390.3 MB 5.3 MB/s eta 0:01:05\n",
      "   ---- ----------------------------------- 48.5/390.3 MB 5.4 MB/s eta 0:01:04\n",
      "   ----- ---------------------------------- 50.6/390.3 MB 5.5 MB/s eta 0:01:02\n",
      "   ----- ---------------------------------- 52.4/390.3 MB 5.6 MB/s eta 0:01:01\n",
      "   ----- ---------------------------------- 54.3/390.3 MB 5.6 MB/s eta 0:01:00\n",
      "   ----- ---------------------------------- 55.8/390.3 MB 5.7 MB/s eta 0:00:59\n",
      "   ----- ---------------------------------- 57.7/390.3 MB 5.7 MB/s eta 0:00:59\n",
      "   ------ --------------------------------- 59.2/390.3 MB 5.8 MB/s eta 0:00:58\n",
      "   ------ --------------------------------- 60.8/390.3 MB 5.8 MB/s eta 0:00:57\n",
      "   ------ --------------------------------- 62.7/390.3 MB 5.9 MB/s eta 0:00:56\n",
      "   ------ --------------------------------- 65.0/390.3 MB 6.0 MB/s eta 0:00:55\n",
      "   ------ --------------------------------- 67.6/390.3 MB 6.1 MB/s eta 0:00:53\n",
      "   ------- -------------------------------- 70.5/390.3 MB 6.2 MB/s eta 0:00:52\n",
      "   ------- -------------------------------- 73.4/390.3 MB 6.4 MB/s eta 0:00:50\n",
      "   ------- -------------------------------- 76.3/390.3 MB 6.5 MB/s eta 0:00:49\n",
      "   -------- ------------------------------- 78.9/390.3 MB 6.6 MB/s eta 0:00:48\n",
      "   -------- ------------------------------- 82.3/390.3 MB 6.8 MB/s eta 0:00:46\n",
      "   -------- ------------------------------- 85.7/390.3 MB 6.9 MB/s eta 0:00:44\n",
      "   --------- ------------------------------ 89.7/390.3 MB 7.1 MB/s eta 0:00:43\n",
      "   --------- ------------------------------ 93.3/390.3 MB 7.3 MB/s eta 0:00:41\n",
      "   --------- ------------------------------ 97.0/390.3 MB 7.5 MB/s eta 0:00:40\n",
      "   ---------- ----------------------------- 101.2/390.3 MB 7.7 MB/s eta 0:00:38\n",
      "   ---------- ----------------------------- 106.2/390.3 MB 7.9 MB/s eta 0:00:36\n",
      "   ----------- ---------------------------- 111.4/390.3 MB 8.2 MB/s eta 0:00:35\n",
      "   ----------- ---------------------------- 116.4/390.3 MB 8.4 MB/s eta 0:00:33\n",
      "   ------------ --------------------------- 121.6/390.3 MB 8.6 MB/s eta 0:00:32\n",
      "   ------------- -------------------------- 126.9/390.3 MB 8.9 MB/s eta 0:00:30\n",
      "   ------------- -------------------------- 133.2/390.3 MB 9.2 MB/s eta 0:00:28\n",
      "   -------------- ------------------------- 138.9/390.3 MB 9.5 MB/s eta 0:00:27\n",
      "   -------------- ------------------------- 145.5/390.3 MB 9.8 MB/s eta 0:00:26\n",
      "   --------------- ----------------------- 152.3/390.3 MB 10.1 MB/s eta 0:00:24\n",
      "   ---------------- ---------------------- 160.4/390.3 MB 10.5 MB/s eta 0:00:22\n",
      "   ---------------- ---------------------- 168.3/390.3 MB 10.8 MB/s eta 0:00:21\n",
      "   ----------------- --------------------- 175.1/390.3 MB 11.1 MB/s eta 0:00:20\n",
      "   ------------------ -------------------- 182.7/390.3 MB 11.5 MB/s eta 0:00:19\n",
      "   ------------------- ------------------- 191.4/390.3 MB 11.8 MB/s eta 0:00:17\n",
      "   -------------------- ------------------ 200.5/390.3 MB 12.2 MB/s eta 0:00:16\n",
      "   --------------------- ----------------- 211.0/390.3 MB 12.7 MB/s eta 0:00:15\n",
      "   ---------------------- ---------------- 221.8/390.3 MB 13.2 MB/s eta 0:00:13\n",
      "   ----------------------- --------------- 231.5/390.3 MB 13.6 MB/s eta 0:00:12\n",
      "   ------------------------ -------------- 240.6/390.3 MB 14.0 MB/s eta 0:00:11\n",
      "   ------------------------- ------------- 251.9/390.3 MB 14.5 MB/s eta 0:00:10\n",
      "   -------------------------- ------------ 261.6/390.3 MB 14.8 MB/s eta 0:00:09\n",
      "   --------------------------- ----------- 273.7/390.3 MB 18.6 MB/s eta 0:00:07\n",
      "   ---------------------------- ---------- 284.4/390.3 MB 21.2 MB/s eta 0:00:06\n",
      "   ----------------------------- --------- 294.9/390.3 MB 23.4 MB/s eta 0:00:05\n",
      "   ------------------------------ -------- 308.5/390.3 MB 26.9 MB/s eta 0:00:04\n",
      "   -------------------------------- ------ 320.9/390.3 MB 30.7 MB/s eta 0:00:03\n",
      "   --------------------------------- ----- 331.9/390.3 MB 34.2 MB/s eta 0:00:02\n",
      "   ---------------------------------- ---- 342.6/390.3 MB 37.2 MB/s eta 0:00:02\n",
      "   ----------------------------------- --- 358.4/390.3 MB 41.1 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 372.2/390.3 MB 44.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  387.7/390.3 MB 47.3 MB/s eta 0:00:01\n",
      "   --------------------------------------  390.1/390.3 MB 47.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  390.1/390.3 MB 47.8 MB/s eta 0:00:01\n",
      "   --------------------------------------- 390.3/390.3 MB 44.4 MB/s eta 0:00:00\n",
      "Downloading absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Downloading flatbuffers-24.3.25-py2.py3-none-any.whl (26 kB)\n",
      "Downloading gast-0.6.0-py3-none-any.whl (21 kB)\n",
      "Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Downloading grpcio-1.68.1-cp312-cp312-win_amd64.whl (4.4 MB)\n",
      "   ---------------------------------------- 0.0/4.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 4.4/4.4 MB 66.6 MB/s eta 0:00:00\n",
      "Downloading keras-3.7.0-py3-none-any.whl (1.2 MB)\n",
      "   ---------------------------------------- 0.0/1.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.2/1.2 MB 60.4 MB/s eta 0:00:00\n",
      "Downloading libclang-18.1.1-py2.py3-none-win_amd64.whl (26.4 MB)\n",
      "   ---------------------------------------- 0.0/26.4 MB ? eta -:--:--\n",
      "   --------------------- ------------------ 13.9/26.4 MB 67.2 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 25.2/26.4 MB 61.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 26.4/26.4 MB 53.9 MB/s eta 0:00:00\n",
      "Downloading ml_dtypes-0.4.1-cp312-cp312-win_amd64.whl (127 kB)\n",
      "Downloading opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
      "Downloading tensorboard-2.18.0-py3-none-any.whl (5.5 MB)\n",
      "   ---------------------------------------- 0.0/5.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 5.5/5.5 MB 56.4 MB/s eta 0:00:00\n",
      "Downloading termcolor-2.5.0-py3-none-any.whl (7.8 kB)\n",
      "Downloading tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)\n",
      "Downloading namex-0.0.8-py3-none-any.whl (5.8 kB)\n",
      "Downloading optree-0.13.1-cp312-cp312-win_amd64.whl (292 kB)\n",
      "Installing collected packages: namex, libclang, flatbuffers, termcolor, tensorboard-data-server, optree, opt-einsum, ml-dtypes, grpcio, google-pasta, gast, astunparse, absl-py, tensorboard, keras, tensorflow-intel, tensorflow\n",
      "Successfully installed absl-py-2.1.0 astunparse-1.6.3 flatbuffers-24.3.25 gast-0.6.0 google-pasta-0.2.0 grpcio-1.68.1 keras-3.7.0 libclang-18.1.1 ml-dtypes-0.4.1 namex-0.0.8 opt-einsum-3.4.0 optree-0.13.1 tensorboard-2.18.0 tensorboard-data-server-0.7.2 tensorflow-2.18.0 tensorflow-intel-2.18.0 termcolor-2.5.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ab48df05-956e-4f0e-a3f4-5d2da1c54605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1913/1913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - loss: 230793.4844 - mse: 230793.4844 - val_loss: 112452.6406 - val_mse: 112452.6406\n",
      "Epoch 2/200\n",
      "\u001b[1m1913/1913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 883us/step - loss: 111251.0312 - mse: 111251.0312 - val_loss: 111909.7656 - val_mse: 111909.7656\n",
      "Epoch 3/200\n",
      "\u001b[1m1913/1913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 888us/step - loss: 109721.7188 - mse: 109721.7188 - val_loss: 111638.7031 - val_mse: 111638.7031\n",
      "Epoch 4/200\n",
      "\u001b[1m1913/1913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 945us/step - loss: 110337.0703 - mse: 110337.0703 - val_loss: 111392.7266 - val_mse: 111392.7266\n",
      "Epoch 5/200\n",
      "\u001b[1m1913/1913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 881us/step - loss: 110484.9062 - mse: 110484.9062 - val_loss: 111629.8047 - val_mse: 111629.8047\n",
      "Epoch 6/200\n",
      "\u001b[1m1913/1913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 911us/step - loss: 109582.4297 - mse: 109582.4297 - val_loss: 111332.6953 - val_mse: 111332.6953\n",
      "Epoch 7/200\n",
      "\u001b[1m1913/1913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 942us/step - loss: 109444.3672 - mse: 109444.3672 - val_loss: 111876.9844 - val_mse: 111876.9844\n",
      "Epoch 8/200\n",
      "\u001b[1m1913/1913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 862us/step - loss: 110082.1406 - mse: 110082.1406 - val_loss: 111524.1797 - val_mse: 111524.1797\n",
      "Epoch 9/200\n",
      "\u001b[1m1913/1913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 921us/step - loss: 109272.6641 - mse: 109272.6641 - val_loss: 111249.3984 - val_mse: 111249.3984\n",
      "Epoch 10/200\n",
      "\u001b[1m1913/1913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 898us/step - loss: 109043.6250 - mse: 109043.6250 - val_loss: 111833.3516 - val_mse: 111833.3516\n",
      "Epoch 11/200\n",
      "\u001b[1m1913/1913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 918us/step - loss: 109451.7344 - mse: 109451.7344 - val_loss: 111541.9219 - val_mse: 111541.9219\n",
      "Epoch 12/200\n",
      "\u001b[1m1913/1913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 926us/step - loss: 109373.7656 - mse: 109373.7656 - val_loss: 111283.9922 - val_mse: 111283.9922\n",
      "Epoch 13/200\n",
      "\u001b[1m1913/1913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 908us/step - loss: 108872.7344 - mse: 108872.7344 - val_loss: 111524.5156 - val_mse: 111524.5156\n",
      "Epoch 14/200\n",
      "\u001b[1m1913/1913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 908us/step - loss: 108970.9375 - mse: 108970.9375 - val_loss: 111396.5469 - val_mse: 111396.5469\n",
      "Epoch 15/200\n",
      "\u001b[1m1913/1913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 987us/step - loss: 109956.1328 - mse: 109956.1328 - val_loss: 111573.7188 - val_mse: 111573.7188\n",
      "Epoch 16/200\n",
      "\u001b[1m1913/1913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 975us/step - loss: 108866.5938 - mse: 108866.5938 - val_loss: 111559.1250 - val_mse: 111559.1250\n",
      "Epoch 17/200\n",
      "\u001b[1m1913/1913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 912us/step - loss: 109263.4922 - mse: 109263.4922 - val_loss: 111409.5391 - val_mse: 111409.5391\n",
      "Epoch 18/200\n",
      "\u001b[1m1913/1913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 941us/step - loss: 110027.6562 - mse: 110027.6562 - val_loss: 111593.3750 - val_mse: 111593.3750\n",
      "Epoch 19/200\n",
      "\u001b[1m1913/1913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 109588.3359 - mse: 109588.3359 - val_loss: 111521.2500 - val_mse: 111521.2500\n",
      "\u001b[1m532/532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 628us/step\n",
      "Optimized Deep Learning Model Evaluation:\n",
      "MSE: 110901.13292466782\n",
      "R²: -0.004535155566714799\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# Build the neural network model\n",
    "model = models.Sequential([\n",
    "    layers.Dense(128, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(1)\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mse'])\n",
    "\n",
    "# Define the early stopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Train the model with early stopping\n",
    "history = model.fit(X_train_scaled, y_train, validation_split=0.1, epochs=200, batch_size=32, \n",
    "                    verbose=1, callbacks=[early_stopping])\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred_nn = model.predict(X_test_scaled).flatten()\n",
    "mse_nn = mean_squared_error(y_test, y_pred_nn)\n",
    "r2_nn = r2_score(y_test, y_pred_nn)\n",
    "\n",
    "print(f\"Optimized Deep Learning Model Evaluation:\\nMSE: {mse_nn}\\nR²: {r2_nn}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2e7b9e14-5c7b-43da-9e73-40e0a344d22e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest MSE: 75850.95538064688, R²: 0.31294704342705637\n",
      "                              Feature    Importance\n",
      "20             days_since_last_review  1.633569e-01\n",
      "5                   reviews per month  1.594750e-01\n",
      "7                    availability 365  1.442803e-01\n",
      "4                   number of reviews  1.230880e-01\n",
      "2                   Construction year  1.131421e-01\n",
      "3                      minimum nights  6.599337e-02\n",
      "6                  review rate number  5.351550e-02\n",
      "19                       review_month  4.544594e-02\n",
      "1                 cancellation_policy  2.823612e-02\n",
      "0              host_identity_verified  1.677581e-02\n",
      "15             room type_Private room  1.675198e-02\n",
      "17              instant_bookable_True  1.603570e-02\n",
      "8        neighbourhood group_Brooklyn  1.497091e-02\n",
      "9       neighbourhood group_Manhattan  1.402143e-02\n",
      "10         neighbourhood group_Queens  1.119641e-02\n",
      "18                        review_year  7.354575e-03\n",
      "16              room type_Shared room  3.998412e-03\n",
      "11  neighbourhood group_Staten Island  2.087739e-03\n",
      "14               room type_Hotel room  2.734929e-04\n",
      "12        neighbourhood group_brookln  4.331211e-07\n",
      "13       neighbourhood group_manhatan  0.000000e+00\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Train the model\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict the price\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
    "r2_rf = r2_score(y_test, y_pred_rf)\n",
    "\n",
    "print(f\"Random Forest MSE: {mse_rf}, R²: {r2_rf}\")\n",
    "\n",
    "# Feature Importance\n",
    "importances = rf_model.feature_importances_\n",
    "feature_importance_df = pd.DataFrame({'Feature': X_train.columns, 'Importance': importances})\n",
    "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "print(feature_importance_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0ee89ace-2d49-4901-84ed-4f2357dc886f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting MSE: 110188.67376975996, R²: 0.0019182525197151135\n",
      "                              Feature  Importance\n",
      "5                   reviews per month    0.225872\n",
      "20             days_since_last_review    0.224425\n",
      "7                    availability 365    0.151287\n",
      "4                   number of reviews    0.121533\n",
      "2                   Construction year    0.081205\n",
      "3                      minimum nights    0.055851\n",
      "19                       review_month    0.033347\n",
      "6                  review rate number    0.025795\n",
      "15             room type_Private room    0.021052\n",
      "9       neighbourhood group_Manhattan    0.011818\n",
      "16              room type_Shared room    0.010302\n",
      "18                        review_year    0.009751\n",
      "10         neighbourhood group_Queens    0.009640\n",
      "1                 cancellation_policy    0.007971\n",
      "14               room type_Hotel room    0.004950\n",
      "17              instant_bookable_True    0.003439\n",
      "0              host_identity_verified    0.001570\n",
      "8        neighbourhood group_Brooklyn    0.000190\n",
      "13       neighbourhood group_manhatan    0.000000\n",
      "12        neighbourhood group_brookln    0.000000\n",
      "11  neighbourhood group_Staten Island    0.000000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# Train the model\n",
    "gb_model = GradientBoostingRegressor(n_estimators=100, random_state=42)\n",
    "gb_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict the price\n",
    "y_pred_gb = gb_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse_gb = mean_squared_error(y_test, y_pred_gb)\n",
    "r2_gb = r2_score(y_test, y_pred_gb)\n",
    "\n",
    "print(f\"Gradient Boosting MSE: {mse_gb}, R²: {r2_gb}\")\n",
    "\n",
    "# Feature Importance\n",
    "importances = gb_model.feature_importances_\n",
    "feature_importance_df = pd.DataFrame({'Feature': X_train.columns, 'Importance': importances})\n",
    "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "print(feature_importance_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4199b686-d163-453c-9c9e-18635858e633",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f87700ad-3374-4c56-bbda-fc9891ecb3e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network MSE: 110523.27739776392, R²: -0.001112565098157603\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "# Train the model\n",
    "nn_model = MLPRegressor(hidden_layer_sizes=(64, 32), max_iter=500, random_state=42)\n",
    "nn_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict the price\n",
    "y_pred_nn = nn_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse_nn = mean_squared_error(y_test, y_pred_nn)\n",
    "r2_nn = r2_score(y_test, y_pred_nn)\n",
    "\n",
    "print(f\"Neural Network MSE: {mse_nn}, R²: {r2_nn}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6e8530d2-e9bd-41c5-85b3-78fd148c87d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost MSE: 105922.51119100528, R²: 0.0405608720918097\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Train the model\n",
    "xgb_model = xgb.XGBRegressor(n_estimators=100, random_state=42)\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict the price\n",
    "y_pred_xgb = xgb_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse_xgb = mean_squared_error(y_test, y_pred_xgb)\n",
    "r2_xgb = r2_score(y_test, y_pred_xgb)\n",
    "\n",
    "print(f\"XGBoost MSE: {mse_xgb}, R²: {r2_xgb}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5b0cdc2b-c503-4bb8-b14a-402cfe1b1a14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboost\n",
      "  Downloading xgboost-2.1.3-py3-none-win_amd64.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: numpy in c:\\programdata\\anaconda3\\lib\\site-packages (from xgboost) (1.26.4)\n",
      "Requirement already satisfied: scipy in c:\\programdata\\anaconda3\\lib\\site-packages (from xgboost) (1.13.1)\n",
      "Downloading xgboost-2.1.3-py3-none-win_amd64.whl (124.9 MB)\n",
      "   ---------------------------------------- 0.0/124.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.3/124.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.0/124.9 MB 2.8 MB/s eta 0:00:45\n",
      "    --------------------------------------- 1.6/124.9 MB 3.0 MB/s eta 0:00:42\n",
      "    --------------------------------------- 2.4/124.9 MB 3.0 MB/s eta 0:00:42\n",
      "   - -------------------------------------- 3.4/124.9 MB 3.4 MB/s eta 0:00:37\n",
      "   - -------------------------------------- 4.5/124.9 MB 3.7 MB/s eta 0:00:33\n",
      "   - -------------------------------------- 5.2/124.9 MB 3.7 MB/s eta 0:00:33\n",
      "   -- ------------------------------------- 6.6/124.9 MB 4.0 MB/s eta 0:00:30\n",
      "   -- ------------------------------------- 8.1/124.9 MB 4.4 MB/s eta 0:00:27\n",
      "   --- ------------------------------------ 9.4/124.9 MB 4.7 MB/s eta 0:00:25\n",
      "   --- ------------------------------------ 11.0/124.9 MB 4.9 MB/s eta 0:00:24\n",
      "   ---- ----------------------------------- 12.8/124.9 MB 5.2 MB/s eta 0:00:22\n",
      "   ---- ----------------------------------- 14.4/124.9 MB 5.4 MB/s eta 0:00:21\n",
      "   ----- ---------------------------------- 16.3/124.9 MB 5.7 MB/s eta 0:00:20\n",
      "   ----- ---------------------------------- 18.4/124.9 MB 5.9 MB/s eta 0:00:19\n",
      "   ------ --------------------------------- 20.2/124.9 MB 6.1 MB/s eta 0:00:18\n",
      "   ------- -------------------------------- 22.5/124.9 MB 6.4 MB/s eta 0:00:17\n",
      "   ------- -------------------------------- 24.6/124.9 MB 6.6 MB/s eta 0:00:16\n",
      "   -------- ------------------------------- 27.0/124.9 MB 6.8 MB/s eta 0:00:15\n",
      "   --------- ------------------------------ 29.1/124.9 MB 7.0 MB/s eta 0:00:14\n",
      "   ---------- ----------------------------- 31.5/124.9 MB 7.2 MB/s eta 0:00:14\n",
      "   ---------- ----------------------------- 34.1/124.9 MB 7.4 MB/s eta 0:00:13\n",
      "   ----------- ---------------------------- 36.4/124.9 MB 7.6 MB/s eta 0:00:12\n",
      "   ------------ --------------------------- 38.5/124.9 MB 7.7 MB/s eta 0:00:12\n",
      "   ------------- -------------------------- 40.9/124.9 MB 7.8 MB/s eta 0:00:11\n",
      "   ------------- -------------------------- 43.0/124.9 MB 7.9 MB/s eta 0:00:11\n",
      "   -------------- ------------------------- 45.6/124.9 MB 8.1 MB/s eta 0:00:10\n",
      "   --------------- ------------------------ 48.2/124.9 MB 8.2 MB/s eta 0:00:10\n",
      "   ---------------- ----------------------- 51.1/124.9 MB 8.4 MB/s eta 0:00:09\n",
      "   ----------------- ---------------------- 54.0/124.9 MB 8.6 MB/s eta 0:00:09\n",
      "   ------------------ --------------------- 57.7/124.9 MB 8.9 MB/s eta 0:00:08\n",
      "   ------------------- -------------------- 60.8/124.9 MB 9.1 MB/s eta 0:00:08\n",
      "   -------------------- ------------------- 63.7/124.9 MB 9.2 MB/s eta 0:00:07\n",
      "   --------------------- ------------------ 67.4/124.9 MB 9.5 MB/s eta 0:00:07\n",
      "   ---------------------- ----------------- 71.0/124.9 MB 9.7 MB/s eta 0:00:06\n",
      "   ----------------------- ---------------- 74.4/124.9 MB 9.9 MB/s eta 0:00:06\n",
      "   ------------------------ --------------- 77.3/124.9 MB 10.0 MB/s eta 0:00:05\n",
      "   ------------------------- -------------- 80.5/124.9 MB 10.1 MB/s eta 0:00:05\n",
      "   --------------------------- ------------ 84.7/124.9 MB 10.4 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 88.6/124.9 MB 10.6 MB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 91.0/124.9 MB 10.6 MB/s eta 0:00:04\n",
      "   ------------------------------ --------- 94.9/124.9 MB 10.8 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 99.4/124.9 MB 11.0 MB/s eta 0:00:03\n",
      "   -------------------------------- ------ 104.1/124.9 MB 11.3 MB/s eta 0:00:02\n",
      "   --------------------------------- ----- 108.5/124.9 MB 11.5 MB/s eta 0:00:02\n",
      "   ----------------------------------- --- 112.5/124.9 MB 11.6 MB/s eta 0:00:02\n",
      "   ------------------------------------ -- 116.9/124.9 MB 11.8 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 121.6/124.9 MB 12.0 MB/s eta 0:00:01\n",
      "   --------------------------------------  124.8/124.9 MB 12.2 MB/s eta 0:00:01\n",
      "   --------------------------------------- 124.9/124.9 MB 12.1 MB/s eta 0:00:00\n",
      "Installing collected packages: xgboost\n",
      "Successfully installed xgboost-2.1.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ee331a03-4a2e-433d-82d5-116ed58d1675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 Score: 0.0405608720918097\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "model = xgb.XGBRegressor(objective='reg:squarederror', random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(f'R^2 Score: {r2_score(y_test, y_pred)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7677d389-ad3e-4e4f-b1cf-44335d789af6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "220e39f1-d48d-4f0a-a1f9-b4a421ce76d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 Score: -0.001112565098157603\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "model = MLPRegressor(hidden_layer_sizes=(64, 32), max_iter=1000, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(f'R^2 Score: {r2_score(y_test, y_pred)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "070b6fc3-460d-4c14-9b45-d0da2cb61940",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 Score: -0.001112565098157603\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "model = MLPRegressor(hidden_layer_sizes=(64, 32), max_iter=1000, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(f'R^2 Score: {r2_score(y_test, y_pred)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cbe46f55-a370-4a3e-9552-ef7cc74a9230",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "poly = PolynomialFeatures(degree=2, interaction_only=True)\n",
    "X_poly = poly.fit_transform(X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "22dabab3-a617-421c-b763-de7ba571e70e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 Score: 0.35391885745449203\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# Base models\n",
    "base_learners = [\n",
    "    ('rf', RandomForestRegressor(n_estimators=100, random_state=42)),\n",
    "    ('gb', GradientBoostingRegressor(n_estimators=100, random_state=42)),\n",
    "    ('lr', Ridge())\n",
    "]\n",
    "model = StackingRegressor(estimators=base_learners, final_estimator=Ridge())\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(f'R^2 Score: {r2_score(y_test, y_pred)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ac0d66fa-0f3c-4dca-9ab6-c5bf0588c9b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 Score: 0.2797427658246271\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=10)  # Reduce to 10 components\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "\n",
    "# Fit a model on the transformed data\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "model.fit(X_train_pca, y_train)\n",
    "y_pred = model.predict(X_test_pca)\n",
    "\n",
    "print(f'R^2 Score: {r2_score(y_test, y_pred)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b1e54854-d931-40c5-a05e-3c9be6d0d8ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2126/2126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 876us/step - loss: 117388.3438\n",
      "Epoch 2/100\n",
      "\u001b[1m2126/2126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 839us/step - loss: 110933.4609\n",
      "Epoch 3/100\n",
      "\u001b[1m2126/2126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 807us/step - loss: 110954.2188\n",
      "Epoch 4/100\n",
      "\u001b[1m2126/2126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 806us/step - loss: 110354.2109\n",
      "Epoch 5/100\n",
      "\u001b[1m2126/2126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 823us/step - loss: 110325.0625\n",
      "Epoch 6/100\n",
      "\u001b[1m2126/2126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 839us/step - loss: 110283.8750\n",
      "Epoch 7/100\n",
      "\u001b[1m2126/2126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 826us/step - loss: 109585.5625\n",
      "Epoch 8/100\n",
      "\u001b[1m2126/2126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 832us/step - loss: 110503.9141\n",
      "Epoch 9/100\n",
      "\u001b[1m2126/2126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 873us/step - loss: 111096.3594\n",
      "Epoch 10/100\n",
      "\u001b[1m2126/2126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 856us/step - loss: 110313.4453\n",
      "Epoch 11/100\n",
      "\u001b[1m2126/2126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 825us/step - loss: 110921.2422\n",
      "Epoch 12/100\n",
      "\u001b[1m2126/2126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 827us/step - loss: 110201.3203\n",
      "Epoch 13/100\n",
      "\u001b[1m2126/2126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 848us/step - loss: 110154.0391\n",
      "Epoch 14/100\n",
      "\u001b[1m2126/2126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 873us/step - loss: 110352.0312\n",
      "Epoch 15/100\n",
      "\u001b[1m2126/2126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 807us/step - loss: 110294.4219\n",
      "Epoch 16/100\n",
      "\u001b[1m2126/2126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 836us/step - loss: 109821.9219\n",
      "Epoch 17/100\n",
      "\u001b[1m2126/2126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 876us/step - loss: 109945.7891\n",
      "Epoch 18/100\n",
      "\u001b[1m2126/2126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 839us/step - loss: 110361.1016\n",
      "Epoch 19/100\n",
      "\u001b[1m2126/2126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 815us/step - loss: 110207.7578\n",
      "Epoch 20/100\n",
      "\u001b[1m2126/2126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 832us/step - loss: 110333.4844\n",
      "Epoch 21/100\n",
      "\u001b[1m2126/2126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 795us/step - loss: 110821.7500\n",
      "Epoch 22/100\n",
      "\u001b[1m2126/2126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 850us/step - loss: 109679.5547\n",
      "Epoch 23/100\n",
      "\u001b[1m2126/2126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 860us/step - loss: 110157.3750\n",
      "Epoch 24/100\n",
      "\u001b[1m2126/2126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 827us/step - loss: 109870.6641\n",
      "Epoch 25/100\n",
      "\u001b[1m2126/2126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 829us/step - loss: 110449.7188\n",
      "Epoch 26/100\n",
      "\u001b[1m2126/2126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 804us/step - loss: 110870.6797\n",
      "Epoch 27/100\n",
      "\u001b[1m2126/2126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 839us/step - loss: 110731.7422\n",
      "Epoch 28/100\n",
      "\u001b[1m2126/2126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 821us/step - loss: 110447.8047\n",
      "Epoch 29/100\n",
      "\u001b[1m2126/2126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 858us/step - loss: 109981.0781\n",
      "Epoch 30/100\n",
      "\u001b[1m2126/2126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 847us/step - loss: 109260.1484\n",
      "Epoch 31/100\n",
      "\u001b[1m2126/2126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 818us/step - loss: 109471.0938\n",
      "Epoch 32/100\n",
      "\u001b[1m2126/2126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 899us/step - loss: 109781.4688\n",
      "Epoch 33/100\n",
      "\u001b[1m2126/2126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 829us/step - loss: 109774.7578\n",
      "Epoch 34/100\n",
      "\u001b[1m2126/2126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 865us/step - loss: 110055.5234\n",
      "Epoch 35/100\n",
      "\u001b[1m2126/2126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 814us/step - loss: 109883.0000\n",
      "Epoch 36/100\n",
      "\u001b[1m2126/2126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 811us/step - loss: 109818.5391\n",
      "Epoch 37/100\n",
      "\u001b[1m2126/2126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 805us/step - loss: 109684.9766\n",
      "Epoch 38/100\n",
      "\u001b[1m2126/2126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 848us/step - loss: 110395.6562\n",
      "Epoch 39/100\n",
      "\u001b[1m2126/2126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 804us/step - loss: 109903.2422\n",
      "Epoch 40/100\n",
      "\u001b[1m2126/2126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 880us/step - loss: 109831.0156\n",
      "Epoch 41/100\n",
      "\u001b[1m2126/2126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 948us/step - loss: 109750.2109\n",
      "Epoch 42/100\n",
      "\u001b[1m2126/2126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 819us/step - loss: 109521.3750\n",
      "Epoch 43/100\n",
      "\u001b[1m2126/2126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 926us/step - loss: 110473.1484\n",
      "Epoch 44/100\n",
      "\u001b[1m2126/2126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 830us/step - loss: 110261.2422\n",
      "Epoch 45/100\n",
      "\u001b[1m2126/2126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 863us/step - loss: 110162.2266\n",
      "Epoch 46/100\n",
      "\u001b[1m2126/2126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 828us/step - loss: 110554.5703\n",
      "Epoch 47/100\n",
      "\u001b[1m2126/2126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 864us/step - loss: 109711.8516\n",
      "Epoch 48/100\n",
      "\u001b[1m2126/2126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 845us/step - loss: 110186.6797\n",
      "Epoch 49/100\n",
      "\u001b[1m2126/2126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 912us/step - loss: 109760.4219\n",
      "Epoch 50/100\n",
      "\u001b[1m2126/2126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 866us/step - loss: 110079.8594\n",
      "Epoch 51/100\n",
      "\u001b[1m2126/2126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 848us/step - loss: 110298.8984\n",
      "Epoch 52/100\n",
      "\u001b[1m2126/2126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 817us/step - loss: 109871.1953\n",
      "Epoch 53/100\n",
      "\u001b[1m2126/2126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 830us/step - loss: 109601.7031\n",
      "Epoch 54/100\n",
      "\u001b[1m2126/2126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 812us/step - loss: 110044.8984\n",
      "Epoch 55/100\n",
      "\u001b[1m2126/2126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 852us/step - loss: 110114.9141\n",
      "Epoch 56/100\n",
      "\u001b[1m2126/2126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 819us/step - loss: 109761.6719\n",
      "Epoch 57/100\n",
      "\u001b[1m2126/2126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 853us/step - loss: 110106.1953\n",
      "Epoch 58/100\n",
      "\u001b[1m2126/2126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 932us/step - loss: 110162.1172\n",
      "Epoch 59/100\n",
      "\u001b[1m2126/2126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 851us/step - loss: 109669.8281\n",
      "Epoch 60/100\n",
      "\u001b[1m2126/2126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 814us/step - loss: 110033.2109\n",
      "Epoch 61/100\n",
      "\u001b[1m2126/2126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 850us/step - loss: 109872.5469\n",
      "Epoch 62/100\n",
      "\u001b[1m2126/2126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 849us/step - loss: 109524.0469\n",
      "Epoch 63/100\n",
      "\u001b[1m2126/2126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 834us/step - loss: 110911.5234\n",
      "Epoch 64/100\n",
      "\u001b[1m2126/2126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 830us/step - loss: 110080.1328\n",
      "Epoch 65/100\n",
      "\u001b[1m2126/2126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 854us/step - loss: 110000.7422\n",
      "Epoch 66/100\n",
      "\u001b[1m2126/2126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 881us/step - loss: 110185.5859\n",
      "Epoch 67/100\n",
      "\u001b[1m2126/2126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 906us/step - loss: 109772.2734\n",
      "Epoch 68/100\n",
      "\u001b[1m2126/2126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 959us/step - loss: 109896.2031\n",
      "Epoch 69/100\n",
      "\u001b[1m2126/2126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 842us/step - loss: 110487.1094\n",
      "Epoch 70/100\n",
      "\u001b[1m2126/2126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 884us/step - loss: 110287.3203\n",
      "Epoch 71/100\n",
      "\u001b[1m2126/2126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 886us/step - loss: 109687.6797\n",
      "Epoch 72/100\n",
      "\u001b[1m2126/2126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 847us/step - loss: 110800.6719\n",
      "Epoch 73/100\n",
      "\u001b[1m2126/2126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 837us/step - loss: 110663.9062\n",
      "Epoch 74/100\n",
      "\u001b[1m2126/2126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 848us/step - loss: 110429.6719\n",
      "Epoch 75/100\n",
      "\u001b[1m2126/2126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 874us/step - loss: 110532.9609\n",
      "Epoch 76/100\n",
      "\u001b[1m2126/2126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 857us/step - loss: 110004.0781\n",
      "Epoch 77/100\n",
      "\u001b[1m2126/2126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 833us/step - loss: 109976.4219\n",
      "Epoch 78/100\n",
      "\u001b[1m2126/2126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 791us/step - loss: 110123.7969\n",
      "Epoch 79/100\n",
      "\u001b[1m2126/2126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 838us/step - loss: 109816.8594\n",
      "Epoch 80/100\n",
      "\u001b[1m2126/2126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 820us/step - loss: 110154.9219\n",
      "Epoch 81/100\n",
      "\u001b[1m2126/2126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 812us/step - loss: 110805.6953\n",
      "Epoch 82/100\n",
      "\u001b[1m2126/2126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 835us/step - loss: 109919.9609\n",
      "Epoch 83/100\n",
      "\u001b[1m2126/2126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 816us/step - loss: 109952.5781\n",
      "Epoch 84/100\n",
      "\u001b[1m2126/2126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 866us/step - loss: 109912.7266\n",
      "Epoch 85/100\n",
      "\u001b[1m2126/2126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 839us/step - loss: 109146.7812\n",
      "Epoch 86/100\n",
      "\u001b[1m2126/2126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 873us/step - loss: 110042.9375\n",
      "Epoch 87/100\n",
      "\u001b[1m2126/2126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 854us/step - loss: 110006.6406\n",
      "Epoch 88/100\n",
      "\u001b[1m2126/2126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 836us/step - loss: 110239.6797\n",
      "Epoch 89/100\n",
      "\u001b[1m2126/2126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 830us/step - loss: 110133.8125\n",
      "Epoch 90/100\n",
      "\u001b[1m2126/2126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 845us/step - loss: 110170.4531\n",
      "Epoch 91/100\n",
      "\u001b[1m2126/2126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 881us/step - loss: 110044.5000\n",
      "Epoch 92/100\n",
      "\u001b[1m2126/2126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 820us/step - loss: 110167.2188\n",
      "Epoch 93/100\n",
      "\u001b[1m2126/2126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 883us/step - loss: 109808.5078\n",
      "Epoch 94/100\n",
      "\u001b[1m2126/2126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 826us/step - loss: 110442.4531\n",
      "Epoch 95/100\n",
      "\u001b[1m2126/2126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 839us/step - loss: 110464.7578\n",
      "Epoch 96/100\n",
      "\u001b[1m2126/2126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 834us/step - loss: 109537.0391\n",
      "Epoch 97/100\n",
      "\u001b[1m2126/2126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 857us/step - loss: 109741.1641\n",
      "Epoch 98/100\n",
      "\u001b[1m2126/2126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 825us/step - loss: 109803.1250\n",
      "Epoch 99/100\n",
      "\u001b[1m2126/2126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 857us/step - loss: 110560.2969\n",
      "Epoch 100/100\n",
      "\u001b[1m2126/2126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 826us/step - loss: 109821.4141\n",
      "\u001b[1m532/532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 604us/step\n",
      "R^2 Score: -0.004638362838733423\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(128, input_dim=X_train.shape[1], activation='relu'),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(1, activation='linear')\n",
    "])\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "model.fit(X_train, y_train, epochs=100, batch_size=32, verbose=1)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "print(f'R^2 Score: {r2_score(y_test, y_pred)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0967a07e-2d25-4a69-af8b-c9c041d66443",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[53], line 19\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Apply RandomizedSearchCV\u001b[39;00m\n\u001b[0;32m     18\u001b[0m random_search \u001b[38;5;241m=\u001b[39m RandomizedSearchCV(model, param_distributions\u001b[38;5;241m=\u001b[39mparam_dist, n_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 19\u001b[0m random_search\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Get best model\u001b[39;00m\n\u001b[0;32m     22\u001b[0m best_model \u001b[38;5;241m=\u001b[39m random_search\u001b[38;5;241m.\u001b[39mbest_estimator_\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1018\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m   1012\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[0;32m   1013\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m   1014\u001b[0m     )\n\u001b[0;32m   1016\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m-> 1018\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_search(evaluate_candidates)\n\u001b[0;32m   1020\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m   1021\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m   1022\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1959\u001b[0m, in \u001b[0;36mRandomizedSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1957\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1958\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1959\u001b[0m     evaluate_candidates(\n\u001b[0;32m   1960\u001b[0m         ParameterSampler(\n\u001b[0;32m   1961\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_distributions, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_iter, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrandom_state\n\u001b[0;32m   1962\u001b[0m         )\n\u001b[0;32m   1963\u001b[0m     )\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:964\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    956\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    957\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m    958\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    959\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    960\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[0;32m    961\u001b[0m         )\n\u001b[0;32m    962\u001b[0m     )\n\u001b[1;32m--> 964\u001b[0m out \u001b[38;5;241m=\u001b[39m parallel(\n\u001b[0;32m    965\u001b[0m     delayed(_fit_and_score)(\n\u001b[0;32m    966\u001b[0m         clone(base_estimator),\n\u001b[0;32m    967\u001b[0m         X,\n\u001b[0;32m    968\u001b[0m         y,\n\u001b[0;32m    969\u001b[0m         train\u001b[38;5;241m=\u001b[39mtrain,\n\u001b[0;32m    970\u001b[0m         test\u001b[38;5;241m=\u001b[39mtest,\n\u001b[0;32m    971\u001b[0m         parameters\u001b[38;5;241m=\u001b[39mparameters,\n\u001b[0;32m    972\u001b[0m         split_progress\u001b[38;5;241m=\u001b[39m(split_idx, n_splits),\n\u001b[0;32m    973\u001b[0m         candidate_progress\u001b[38;5;241m=\u001b[39m(cand_idx, n_candidates),\n\u001b[0;32m    974\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_and_score_kwargs,\n\u001b[0;32m    975\u001b[0m     )\n\u001b[0;32m    976\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m (cand_idx, parameters), (split_idx, (train, test)) \u001b[38;5;129;01min\u001b[39;00m product(\n\u001b[0;32m    977\u001b[0m         \u001b[38;5;28menumerate\u001b[39m(candidate_params),\n\u001b[0;32m    978\u001b[0m         \u001b[38;5;28menumerate\u001b[39m(cv\u001b[38;5;241m.\u001b[39msplit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mrouted_params\u001b[38;5;241m.\u001b[39msplitter\u001b[38;5;241m.\u001b[39msplit)),\n\u001b[0;32m    979\u001b[0m     )\n\u001b[0;32m    980\u001b[0m )\n\u001b[0;32m    982\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    983\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    984\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    985\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    986\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    987\u001b[0m     )\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\parallel.py:74\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     69\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     70\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     71\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     73\u001b[0m )\n\u001b[1;32m---> 74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(iterable_with_config)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[0;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[0;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[0;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:1762\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m   1760\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[0;32m   1761\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[1;32m-> 1762\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.01\u001b[39m)\n\u001b[0;32m   1763\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[0;32m   1766\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[0;32m   1767\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from scipy.stats import randint\n",
    "\n",
    "# Define model\n",
    "model = RandomForestRegressor()\n",
    "\n",
    "# Define hyperparameter space with corrected 'max_features'\n",
    "param_dist = {\n",
    "    'n_estimators': randint(100, 1000),\n",
    "    'max_features': ['sqrt', 'log2', None],  # Corrected this line\n",
    "    'max_depth': randint(1, 20),\n",
    "    'min_samples_split': randint(2, 10),\n",
    "    'min_samples_leaf': randint(1, 10)\n",
    "}\n",
    "\n",
    "# Apply RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(model, param_distributions=param_dist, n_iter=50, random_state=42, n_jobs=-1)\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Get best model\n",
    "best_model = random_search.best_estimator_\n",
    "print(best_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3ca88989-3422-4776-8d84-4df3dc43c15f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestRegressor(max_depth=17, max_features=None, min_samples_leaf=3,\n",
      "                      min_samples_split=5, n_estimators=666)\n"
     ]
    }
   ],
   "source": [
    "print(best_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8bf21967-ab94-497e-a182-18934fd8dc8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacking Regressor R^2 Score: 0.21211268025798147\n",
      "Voting Regressor R^2 Score: 0.027502923193305273\n",
      "Optimized RandomForestRegressor: RandomForestRegressor(max_depth=17, max_features=None, min_samples_leaf=3,\n",
      "                      min_samples_split=5, n_estimators=666, random_state=42)\n",
      "Optimized RandomForest R^2 Score: 0.0815121287804611\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import StackingRegressor, VotingRegressor, RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from scipy.stats import randint\n",
    "\n",
    "# Splitting the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define optimized Random Forest model\n",
    "optimized_rf = RandomForestRegressor(\n",
    "    max_depth=17,\n",
    "    max_features=None,\n",
    "    min_samples_leaf=3,\n",
    "    min_samples_split=5,\n",
    "    n_estimators=666,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Base models for stacking and voting\n",
    "base_learners = [\n",
    "    ('rf', optimized_rf),\n",
    "    ('gb', GradientBoostingRegressor(n_estimators=100, random_state=42)),\n",
    "    ('xgb', XGBRegressor(n_estimators=100, learning_rate=0.1, random_state=42)),\n",
    "    ('lr', Ridge())\n",
    "]\n",
    "\n",
    "# Stacking Regressor\n",
    "stacking_model = StackingRegressor(estimators=base_learners, final_estimator=Ridge())\n",
    "stacking_model.fit(X_train, y_train)\n",
    "stacking_y_pred = stacking_model.predict(X_test)\n",
    "stacking_r2 = r2_score(y_test, stacking_y_pred)\n",
    "print(f'Stacking Regressor R^2 Score: {stacking_r2}')\n",
    "\n",
    "# Voting Regressor\n",
    "voting_model = VotingRegressor(estimators=base_learners)\n",
    "voting_model.fit(X_train, y_train)\n",
    "voting_y_pred = voting_model.predict(X_test)\n",
    "voting_r2 = r2_score(y_test, voting_y_pred)\n",
    "print(f'Voting Regressor R^2 Score: {voting_r2}')\n",
    "\n",
    "# Hyperparameter tuning for RandomForest\n",
    "param_dist = {\n",
    "    'n_estimators': randint(100, 1000),\n",
    "    'max_features': ['sqrt', 'log2', None],\n",
    "    'max_depth': randint(1, 20),\n",
    "    'min_samples_split': randint(2, 10),\n",
    "    'min_samples_leaf': randint(1, 10)\n",
    "}\n",
    "\n",
    "random_search = RandomizedSearchCV(RandomForestRegressor(random_state=42), param_distributions=param_dist, \n",
    "                                   n_iter=50, random_state=42, n_jobs=-1)\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Get best RandomForest model from RandomizedSearchCV\n",
    "best_rf = random_search.best_estimator_\n",
    "print(f'Optimized RandomForestRegressor: {best_rf}')\n",
    "\n",
    "# Evaluate the best RF model\n",
    "rf_y_pred = best_rf.predict(X_test)\n",
    "rf_r2 = r2_score(y_test, rf_y_pred)\n",
    "print(f'Optimized RandomForest R^2 Score: {rf_r2}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4c1e3cbd-70d4-45a9-884c-dc322c540706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacking Regressor R^2 Score (Improved): 0.23241462809994806\n",
      "Cross-Validation R^2 Scores: [ 0.2553755   0.31268431 -0.21025603  0.28602443  0.47723312]\n",
      "Mean Cross-Validation R^2: 0.2242122677765887\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import StackingRegressor, RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Pipeline for scaling and stacking\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Base models with tuned parameters\n",
    "base_learners = [\n",
    "    ('rf', RandomForestRegressor(max_depth=17, n_estimators=666, min_samples_leaf=3, min_samples_split=5, random_state=42)),\n",
    "    ('gb', GradientBoostingRegressor(n_estimators=200, learning_rate=0.05, max_depth=5, random_state=42)),\n",
    "    ('xgb', XGBRegressor(n_estimators=200, learning_rate=0.05, max_depth=5, random_state=42))\n",
    "]\n",
    "\n",
    "# Stacking with a stronger final estimator\n",
    "stacking_model = StackingRegressor(\n",
    "    estimators=base_learners,\n",
    "    final_estimator=XGBRegressor(n_estimators=200, learning_rate=0.05, random_state=42)\n",
    ")\n",
    "\n",
    "# Fit and predict\n",
    "stacking_model.fit(X_train, y_train)\n",
    "y_pred = stacking_model.predict(X_test)\n",
    "\n",
    "# Evaluate with R^2\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f'Stacking Regressor R^2 Score (Improved): {r2}')\n",
    "\n",
    "# Cross-validation for robust evaluation\n",
    "cv_scores = cross_val_score(stacking_model, X, y, cv=5, scoring='r2')\n",
    "print(f'Cross-Validation R^2 Scores: {cv_scores}')\n",
    "print(f'Mean Cross-Validation R^2: {cv_scores.mean()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "76f4e75b-ff96-47a5-b92a-ebd6c5f5c429",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Evaluation:\n",
      "MSE: 101401.47403636416\n",
      "R²: 0.0815121287804611\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestRegressor(max_depth=17, n_estimators=666, min_samples_leaf=3, min_samples_split=5, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict prices\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f\"Model Evaluation:\\nMSE: {mse}\\nR²: {r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204bcc9a-238c-4956-bf23-c06b676d7702",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
